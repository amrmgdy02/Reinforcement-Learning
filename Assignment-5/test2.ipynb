{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e50ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "âœ… Successfully created environment: SpaceInvadersNoFrameskip-v4\n",
      "   Action space: Discrete(6)\n",
      "   Observation space: Box(0, 255, (210, 160, 3), uint8)\n",
      "ðŸ“ Preprocessed observation shape: (3, 64, 64)\n",
      "ðŸ“ Encoder output dimension: 1024\n",
      "Episode 0: Reward = 335.0, Avg (last 10) = 335.0, Steps = 2301\n",
      "  Time: 63.3s/ep, Remaining: 17.6h\n",
      "ðŸ’¾ New best model saved! Reward: 335.0\n",
      "Episode 1: Reward = 180.0, Avg (last 10) = 180.0, Steps = 4618\n",
      "  Time: 84.9s/ep, Remaining: 23.5h\n",
      "Episode 2: Reward = 215.0, Avg (last 10) = 215.0, Steps = 7311\n",
      "  Time: 97.3s/ep, Remaining: 27.0h\n",
      "Episode 3: Reward = 385.0, Avg (last 10) = 385.0, Steps = 9864\n",
      "  Time: 102.7s/ep, Remaining: 28.4h\n",
      "ðŸ’¾ New best model saved! Reward: 385.0\n",
      "Episode 4: Reward = 285.0, Avg (last 10) = 285.0, Steps = 13977\n",
      "  Time: 120.1s/ep, Remaining: 33.2h\n",
      "Episode 5: Reward = 50.0, Avg (last 10) = 50.0, Steps = 15478\n",
      "  Time: 111.5s/ep, Remaining: 30.8h\n",
      "Episode 6: Reward = 155.0, Avg (last 10) = 155.0, Steps = 17891\n",
      "  Time: 111.6s/ep, Remaining: 30.8h\n",
      "Episode 7: Reward = 210.0, Avg (last 10) = 210.0, Steps = 20356\n",
      "  Time: 111.8s/ep, Remaining: 30.8h\n",
      "Episode 8: Reward = 440.0, Avg (last 10) = 440.0, Steps = 23203\n",
      "  Time: 113.9s/ep, Remaining: 31.4h\n",
      "ðŸ’¾ New best model saved! Reward: 440.0\n",
      "Episode 9: Reward = 180.0, Avg (last 10) = 243.5, Steps = 25550\n",
      "  Time: 113.4s/ep, Remaining: 31.2h\n",
      "Episode 10: Reward = 180.0, Avg (last 10) = 228.0, Steps = 27927\n",
      "  Time: 113.1s/ep, Remaining: 31.1h\n",
      "Episode 11: Reward = 75.0, Avg (last 10) = 217.5, Steps = 29116\n",
      "  Time: 108.2s/ep, Remaining: 29.7h\n",
      "Episode 12: Reward = 210.0, Avg (last 10) = 217.0, Steps = 31539\n",
      "  Time: 108.4s/ep, Remaining: 29.7h\n",
      "Episode 13: Reward = 75.0, Avg (last 10) = 186.0, Steps = 33146\n",
      "  Time: 106.0s/ep, Remaining: 29.0h\n",
      "Episode 14: Reward = 380.0, Avg (last 10) = 195.5, Steps = 35561\n",
      "  Time: 106.4s/ep, Remaining: 29.1h\n",
      "Episode 15: Reward = 180.0, Avg (last 10) = 208.5, Steps = 37966\n",
      "  Time: 106.6s/ep, Remaining: 29.1h\n",
      "Episode 16: Reward = 210.0, Avg (last 10) = 214.0, Steps = 40453\n",
      "  Time: 107.1s/ep, Remaining: 29.2h\n",
      "Episode 17: Reward = 210.0, Avg (last 10) = 214.0, Steps = 42890\n",
      "  Time: 107.4s/ep, Remaining: 29.3h\n",
      "Episode 18: Reward = 110.0, Avg (last 10) = 181.0, Steps = 44803\n",
      "  Time: 106.3s/ep, Remaining: 29.0h\n",
      "Episode 19: Reward = 105.0, Avg (last 10) = 173.5, Steps = 46708\n",
      "  Time: 105.3s/ep, Remaining: 28.7h\n",
      "Episode 20: Reward = 285.0, Avg (last 10) = 184.0, Steps = 49645\n",
      "  Time: 106.9s/ep, Remaining: 29.1h\n",
      "Episode 21: Reward = 380.0, Avg (last 10) = 214.5, Steps = 52192\n",
      "  Time: 107.4s/ep, Remaining: 29.2h\n",
      "Episode 22: Reward = 155.0, Avg (last 10) = 209.0, Steps = 54657\n",
      "  Time: 107.6s/ep, Remaining: 29.2h\n",
      "Episode 23: Reward = 180.0, Avg (last 10) = 219.5, Steps = 57070\n",
      "  Time: 107.8s/ep, Remaining: 29.2h\n",
      "Episode 24: Reward = 75.0, Avg (last 10) = 189.0, Steps = 58257\n",
      "  Time: 105.7s/ep, Remaining: 28.6h\n",
      "Episode 25: Reward = 180.0, Avg (last 10) = 189.0, Steps = 60702\n",
      "  Time: 105.9s/ep, Remaining: 28.7h\n",
      "Episode 26: Reward = 75.0, Avg (last 10) = 175.5, Steps = 62627\n",
      "  Time: 105.3s/ep, Remaining: 28.5h\n",
      "Episode 27: Reward = 210.0, Avg (last 10) = 175.5, Steps = 65060\n",
      "  Time: 105.6s/ep, Remaining: 28.5h\n",
      "Episode 28: Reward = 180.0, Avg (last 10) = 182.5, Steps = 67555\n",
      "  Time: 105.9s/ep, Remaining: 28.6h\n",
      "Episode 29: Reward = 460.0, Avg (last 10) = 218.0, Steps = 70846\n",
      "  Time: 107.4s/ep, Remaining: 28.9h\n",
      "ðŸ’¾ New best model saved! Reward: 460.0\n",
      "Episode 30: Reward = 260.0, Avg (last 10) = 215.5, Steps = 73703\n",
      "  Time: 108.2s/ep, Remaining: 29.1h\n",
      "Episode 31: Reward = 50.0, Avg (last 10) = 182.5, Steps = 75280\n",
      "  Time: 107.1s/ep, Remaining: 28.8h\n",
      "Episode 32: Reward = 110.0, Avg (last 10) = 178.0, Steps = 77023\n",
      "  Time: 106.3s/ep, Remaining: 28.6h\n",
      "Episode 33: Reward = 155.0, Avg (last 10) = 175.5, Steps = 79410\n",
      "  Time: 106.4s/ep, Remaining: 28.6h\n",
      "Episode 34: Reward = 150.0, Avg (last 10) = 183.0, Steps = 81735\n",
      "  Time: 106.5s/ep, Remaining: 28.5h\n",
      "Episode 35: Reward = 180.0, Avg (last 10) = 183.0, Steps = 84186\n",
      "  Time: 106.6s/ep, Remaining: 28.6h\n",
      "Episode 36: Reward = 255.0, Avg (last 10) = 201.0, Steps = 87181\n",
      "  Time: 107.5s/ep, Remaining: 28.8h\n",
      "Episode 37: Reward = 115.0, Avg (last 10) = 191.5, Steps = 89516\n",
      "  Time: 107.5s/ep, Remaining: 28.7h\n",
      "Episode 38: Reward = 155.0, Avg (last 10) = 189.0, Steps = 91787\n",
      "  Time: 107.5s/ep, Remaining: 28.7h\n",
      "Episode 39: Reward = 120.0, Avg (last 10) = 155.0, Steps = 93826\n",
      "  Time: 107.1s/ep, Remaining: 28.6h\n",
      "Episode 40: Reward = 135.0, Avg (last 10) = 142.5, Steps = 95847\n",
      "  Time: 106.8s/ep, Remaining: 28.5h\n",
      "Episode 41: Reward = 490.0, Avg (last 10) = 186.5, Steps = 99204\n",
      "  Time: 108.0s/ep, Remaining: 28.7h\n",
      "ðŸ’¾ New best model saved! Reward: 490.0\n",
      "Episode 42: Reward = 240.0, Avg (last 10) = 199.5, Steps = 102113\n",
      "  Time: 108.6s/ep, Remaining: 28.9h\n",
      "Episode 43: Reward = 155.0, Avg (last 10) = 199.5, Steps = 104470\n",
      "  Time: 108.6s/ep, Remaining: 28.8h\n",
      "Episode 44: Reward = 135.0, Avg (last 10) = 198.0, Steps = 106769\n",
      "  Time: 108.5s/ep, Remaining: 28.8h\n",
      "Episode 45: Reward = 155.0, Avg (last 10) = 195.5, Steps = 109176\n",
      "  Time: 108.6s/ep, Remaining: 28.8h\n",
      "Episode 46: Reward = 240.0, Avg (last 10) = 194.0, Steps = 112541\n",
      "  Time: 109.6s/ep, Remaining: 29.0h\n",
      "Episode 47: Reward = 135.0, Avg (last 10) = 196.0, Steps = 114898\n",
      "  Time: 109.6s/ep, Remaining: 29.0h\n",
      "Episode 48: Reward = 155.0, Avg (last 10) = 196.0, Steps = 117253\n",
      "  Time: 109.6s/ep, Remaining: 28.9h\n",
      "Episode 49: Reward = 210.0, Avg (last 10) = 205.0, Steps = 119734\n",
      "  Time: 109.7s/ep, Remaining: 28.9h\n",
      "Episode 50: Reward = 110.0, Avg (last 10) = 202.5, Steps = 121731\n",
      "  Time: 109.4s/ep, Remaining: 28.8h\n",
      "ðŸ’¾ Checkpoint saved at episode 50\n",
      "Episode 51: Reward = 110.0, Avg (last 10) = 164.5, Steps = 123660\n",
      "  Time: 108.9s/ep, Remaining: 28.7h\n",
      "Episode 52: Reward = 210.0, Avg (last 10) = 161.5, Steps = 126185\n",
      "  Time: 109.1s/ep, Remaining: 28.7h\n",
      "Episode 53: Reward = 110.0, Avg (last 10) = 157.0, Steps = 128138\n",
      "  Time: 108.8s/ep, Remaining: 28.6h\n",
      "Episode 54: Reward = 110.0, Avg (last 10) = 154.5, Steps = 130367\n",
      "  Time: 108.6s/ep, Remaining: 28.5h\n",
      "Episode 55: Reward = 75.0, Avg (last 10) = 146.5, Steps = 131592\n",
      "  Time: 109.5s/ep, Remaining: 28.7h\n",
      "Episode 56: Reward = 75.0, Avg (last 10) = 130.0, Steps = 133061\n",
      "  Time: 109.8s/ep, Remaining: 28.8h\n",
      "Episode 57: Reward = 105.0, Avg (last 10) = 127.0, Steps = 134894\n",
      "  Time: 109.4s/ep, Remaining: 28.6h\n",
      "Episode 58: Reward = 210.0, Avg (last 10) = 132.5, Steps = 137355\n",
      "  Time: 109.5s/ep, Remaining: 28.6h\n",
      "Episode 59: Reward = 355.0, Avg (last 10) = 147.0, Steps = 139814\n",
      "  Time: 109.5s/ep, Remaining: 28.6h\n",
      "Episode 60: Reward = 110.0, Avg (last 10) = 147.0, Steps = 141903\n",
      "  Time: 109.3s/ep, Remaining: 28.5h\n",
      "Episode 61: Reward = 150.0, Avg (last 10) = 151.0, Steps = 144214\n",
      "  Time: 109.2s/ep, Remaining: 28.5h\n",
      "Episode 62: Reward = 180.0, Avg (last 10) = 148.0, Steps = 146335\n",
      "  Time: 109.1s/ep, Remaining: 28.4h\n",
      "Episode 63: Reward = 80.0, Avg (last 10) = 145.0, Steps = 147870\n",
      "  Time: 108.5s/ep, Remaining: 28.2h\n",
      "Episode 64: Reward = 140.0, Avg (last 10) = 148.0, Steps = 150255\n",
      "  Time: 108.6s/ep, Remaining: 28.2h\n",
      "Episode 65: Reward = 60.0, Avg (last 10) = 146.5, Steps = 151696\n",
      "  Time: 108.1s/ep, Remaining: 28.0h\n",
      "Episode 66: Reward = 330.0, Avg (last 10) = 172.0, Steps = 154733\n",
      "  Time: 108.6s/ep, Remaining: 28.2h\n",
      "Episode 67: Reward = 120.0, Avg (last 10) = 173.5, Steps = 156732\n",
      "  Time: 108.4s/ep, Remaining: 28.1h\n",
      "Episode 68: Reward = 75.0, Avg (last 10) = 160.0, Steps = 158601\n",
      "  Time: 108.1s/ep, Remaining: 28.0h\n",
      "Episode 69: Reward = 105.0, Avg (last 10) = 135.0, Steps = 160532\n",
      "  Time: 107.9s/ep, Remaining: 27.9h\n",
      "Episode 70: Reward = 155.0, Avg (last 10) = 139.5, Steps = 162899\n",
      "  Time: 108.0s/ep, Remaining: 27.9h\n",
      "Episode 71: Reward = 135.0, Avg (last 10) = 138.0, Steps = 165326\n",
      "  Time: 108.1s/ep, Remaining: 27.9h\n",
      "Episode 72: Reward = 180.0, Avg (last 10) = 138.0, Steps = 167773\n",
      "  Time: 108.2s/ep, Remaining: 27.9h\n",
      "Episode 73: Reward = 150.0, Avg (last 10) = 145.0, Steps = 169696\n",
      "  Time: 108.0s/ep, Remaining: 27.8h\n",
      "Episode 74: Reward = 425.0, Avg (last 10) = 173.5, Steps = 172527\n",
      "  Time: 108.3s/ep, Remaining: 27.8h\n",
      "Episode 75: Reward = 410.0, Avg (last 10) = 208.5, Steps = 175050\n",
      "  Time: 108.4s/ep, Remaining: 27.8h\n",
      "Episode 76: Reward = 105.0, Avg (last 10) = 186.0, Steps = 176973\n",
      "  Time: 108.2s/ep, Remaining: 27.7h\n",
      "Episode 77: Reward = 120.0, Avg (last 10) = 186.0, Steps = 179286\n",
      "  Time: 108.3s/ep, Remaining: 27.7h\n",
      "Episode 78: Reward = 180.0, Avg (last 10) = 196.5, Steps = 181821\n",
      "  Time: 108.4s/ep, Remaining: 27.7h\n",
      "Episode 79: Reward = 135.0, Avg (last 10) = 199.5, Steps = 184162\n",
      "  Time: 108.4s/ep, Remaining: 27.7h\n",
      "Episode 80: Reward = 260.0, Avg (last 10) = 210.0, Steps = 187033\n",
      "  Time: 108.8s/ep, Remaining: 27.8h\n",
      "Episode 81: Reward = 210.0, Avg (last 10) = 217.5, Steps = 189524\n",
      "  Time: 108.9s/ep, Remaining: 27.8h\n",
      "Episode 82: Reward = 345.0, Avg (last 10) = 234.0, Steps = 191879\n",
      "  Time: 109.0s/ep, Remaining: 27.8h\n",
      "Episode 83: Reward = 75.0, Avg (last 10) = 226.5, Steps = 193844\n",
      "  Time: 108.8s/ep, Remaining: 27.7h\n",
      "Episode 84: Reward = 210.0, Avg (last 10) = 205.0, Steps = 196293\n",
      "  Time: 108.8s/ep, Remaining: 27.7h\n",
      "Episode 85: Reward = 420.0, Avg (last 10) = 206.0, Steps = 199222\n",
      "  Time: 109.2s/ep, Remaining: 27.7h\n",
      "Episode 86: Reward = 160.0, Avg (last 10) = 211.5, Steps = 201517\n",
      "  Time: 109.2s/ep, Remaining: 27.7h\n",
      "Episode 87: Reward = 180.0, Avg (last 10) = 217.5, Steps = 204062\n",
      "  Time: 109.3s/ep, Remaining: 27.7h\n",
      "Episode 88: Reward = 285.0, Avg (last 10) = 228.0, Steps = 206951\n",
      "  Time: 109.6s/ep, Remaining: 27.7h\n",
      "Episode 89: Reward = 410.0, Avg (last 10) = 255.5, Steps = 209512\n",
      "  Time: 109.7s/ep, Remaining: 27.7h\n",
      "Episode 90: Reward = 135.0, Avg (last 10) = 243.0, Steps = 211913\n",
      "  Time: 109.8s/ep, Remaining: 27.7h\n",
      "Episode 91: Reward = 210.0, Avg (last 10) = 243.0, Steps = 214314\n",
      "  Time: 110.0s/ep, Remaining: 27.7h\n",
      "Episode 92: Reward = 120.0, Avg (last 10) = 220.5, Steps = 216643\n",
      "  Time: 110.0s/ep, Remaining: 27.7h\n",
      "Episode 93: Reward = 210.0, Avg (last 10) = 234.0, Steps = 219212\n",
      "  Time: 110.2s/ep, Remaining: 27.7h\n",
      "Episode 94: Reward = 310.0, Avg (last 10) = 244.0, Steps = 221499\n",
      "  Time: 110.2s/ep, Remaining: 27.7h\n",
      "Episode 95: Reward = 210.0, Avg (last 10) = 223.0, Steps = 224034\n",
      "  Time: 110.3s/ep, Remaining: 27.7h\n",
      "Episode 96: Reward = 50.0, Avg (last 10) = 212.0, Steps = 225237\n",
      "  Time: 109.8s/ep, Remaining: 27.5h\n",
      "Episode 97: Reward = 180.0, Avg (last 10) = 212.0, Steps = 227630\n",
      "  Time: 109.8s/ep, Remaining: 27.5h\n",
      "Episode 98: Reward = 210.0, Avg (last 10) = 204.5, Steps = 230171\n",
      "  Time: 109.9s/ep, Remaining: 27.5h\n",
      "Episode 99: Reward = 180.0, Avg (last 10) = 181.5, Steps = 232636\n",
      "  Time: 110.0s/ep, Remaining: 27.5h\n",
      "Episode 100: Reward = 165.0, Avg (last 10) = 184.5, Steps = 235011\n",
      "  Time: 110.1s/ep, Remaining: 27.5h\n",
      "ðŸ’¾ Checkpoint saved at episode 100\n",
      "Episode 101: Reward = 110.0, Avg (last 10) = 174.5, Steps = 237290\n",
      "  Time: 110.1s/ep, Remaining: 27.5h\n",
      "Episode 102: Reward = 240.0, Avg (last 10) = 186.5, Steps = 240161\n",
      "  Time: 110.3s/ep, Remaining: 27.5h\n",
      "Episode 103: Reward = 225.0, Avg (last 10) = 188.0, Steps = 243040\n",
      "  Time: 110.6s/ep, Remaining: 27.5h\n",
      "Episode 104: Reward = 140.0, Avg (last 10) = 171.0, Steps = 245407\n",
      "  Time: 110.6s/ep, Remaining: 27.5h\n",
      "Episode 105: Reward = 250.0, Avg (last 10) = 175.0, Steps = 248256\n",
      "  Time: 110.8s/ep, Remaining: 27.5h\n",
      "Episode 106: Reward = 380.0, Avg (last 10) = 208.0, Steps = 250747\n",
      "  Time: 110.9s/ep, Remaining: 27.5h\n",
      "Episode 107: Reward = 110.0, Avg (last 10) = 201.0, Steps = 252696\n",
      "  Time: 110.7s/ep, Remaining: 27.4h\n",
      "Episode 108: Reward = 135.0, Avg (last 10) = 193.5, Steps = 255051\n",
      "  Time: 110.8s/ep, Remaining: 27.4h\n",
      "Episode 109: Reward = 155.0, Avg (last 10) = 191.0, Steps = 257542\n",
      "  Time: 110.9s/ep, Remaining: 27.4h\n",
      "Episode 110: Reward = 210.0, Avg (last 10) = 195.5, Steps = 260053\n",
      "  Time: 111.0s/ep, Remaining: 27.4h\n",
      "Episode 111: Reward = 155.0, Avg (last 10) = 200.0, Steps = 262488\n",
      "  Time: 111.0s/ep, Remaining: 27.4h\n",
      "Episode 112: Reward = 180.0, Avg (last 10) = 194.0, Steps = 264951\n",
      "  Time: 111.1s/ep, Remaining: 27.4h\n",
      "Episode 113: Reward = 135.0, Avg (last 10) = 185.0, Steps = 267314\n",
      "  Time: 111.1s/ep, Remaining: 27.3h\n",
      "Episode 114: Reward = 105.0, Avg (last 10) = 181.5, Steps = 269249\n",
      "  Time: 110.9s/ep, Remaining: 27.3h\n",
      "Episode 115: Reward = 50.0, Avg (last 10) = 161.5, Steps = 270750\n",
      "  Time: 110.6s/ep, Remaining: 27.1h\n",
      "Episode 116: Reward = 75.0, Avg (last 10) = 131.0, Steps = 272057\n",
      "  Time: 110.1s/ep, Remaining: 27.0h\n",
      "Episode 117: Reward = 355.0, Avg (last 10) = 155.5, Steps = 274552\n",
      "  Time: 110.2s/ep, Remaining: 27.0h\n",
      "Episode 118: Reward = 110.0, Avg (last 10) = 153.0, Steps = 276555\n",
      "  Time: 110.1s/ep, Remaining: 26.9h\n",
      "Episode 119: Reward = 75.0, Avg (last 10) = 145.0, Steps = 278366\n",
      "  Time: 109.9s/ep, Remaining: 26.9h\n",
      "Episode 120: Reward = 110.0, Avg (last 10) = 135.0, Steps = 280415\n",
      "  Time: 109.8s/ep, Remaining: 26.8h\n",
      "Episode 121: Reward = 225.0, Avg (last 10) = 142.0, Steps = 283236\n",
      "  Time: 110.0s/ep, Remaining: 26.8h\n",
      "Episode 122: Reward = 155.0, Avg (last 10) = 139.5, Steps = 285371\n",
      "  Time: 109.9s/ep, Remaining: 26.8h\n",
      "Episode 123: Reward = 210.0, Avg (last 10) = 147.0, Steps = 287814\n",
      "  Time: 110.0s/ep, Remaining: 26.8h\n",
      "Episode 124: Reward = 210.0, Avg (last 10) = 157.5, Steps = 290297\n",
      "  Time: 110.0s/ep, Remaining: 26.7h\n",
      "Episode 125: Reward = 210.0, Avg (last 10) = 173.5, Steps = 292878\n",
      "  Time: 110.1s/ep, Remaining: 26.7h\n",
      "Episode 126: Reward = 105.0, Avg (last 10) = 176.5, Steps = 294985\n",
      "  Time: 110.1s/ep, Remaining: 26.7h\n",
      "Episode 127: Reward = 140.0, Avg (last 10) = 155.0, Steps = 297076\n",
      "  Time: 110.0s/ep, Remaining: 26.6h\n",
      "Episode 128: Reward = 440.0, Avg (last 10) = 188.0, Steps = 299893\n",
      "  Time: 110.2s/ep, Remaining: 26.7h\n",
      "Episode 129: Reward = 530.0, Avg (last 10) = 233.5, Steps = 303266\n",
      "  Time: 110.6s/ep, Remaining: 26.7h\n",
      "ðŸ’¾ New best model saved! Reward: 530.0\n",
      "Episode 130: Reward = 155.0, Avg (last 10) = 238.0, Steps = 305687\n",
      "  Time: 110.6s/ep, Remaining: 26.7h\n",
      "Episode 131: Reward = 240.0, Avg (last 10) = 239.5, Steps = 308586\n",
      "  Time: 110.8s/ep, Remaining: 26.7h\n",
      "Episode 132: Reward = 155.0, Avg (last 10) = 239.5, Steps = 311067\n",
      "  Time: 110.9s/ep, Remaining: 26.7h\n",
      "Episode 133: Reward = 310.0, Avg (last 10) = 249.5, Steps = 313342\n",
      "  Time: 110.8s/ep, Remaining: 26.7h\n",
      "Episode 134: Reward = 105.0, Avg (last 10) = 239.0, Steps = 315363\n",
      "  Time: 110.7s/ep, Remaining: 26.6h\n",
      "Episode 135: Reward = 135.0, Avg (last 10) = 231.5, Steps = 317716\n",
      "  Time: 110.7s/ep, Remaining: 26.6h\n",
      "Episode 136: Reward = 105.0, Avg (last 10) = 231.5, Steps = 319781\n",
      "  Time: 110.6s/ep, Remaining: 26.5h\n",
      "Episode 137: Reward = 150.0, Avg (last 10) = 232.5, Steps = 321906\n",
      "  Time: 110.6s/ep, Remaining: 26.5h\n",
      "Episode 138: Reward = 295.0, Avg (last 10) = 218.0, Steps = 325117\n",
      "  Time: 110.9s/ep, Remaining: 26.5h\n",
      "Episode 139: Reward = 180.0, Avg (last 10) = 183.0, Steps = 327640\n",
      "  Time: 110.9s/ep, Remaining: 26.5h\n",
      "Episode 140: Reward = 210.0, Avg (last 10) = 188.5, Steps = 330031\n",
      "  Time: 110.9s/ep, Remaining: 26.5h\n",
      "Episode 141: Reward = 135.0, Avg (last 10) = 178.0, Steps = 332440\n",
      "  Time: 111.0s/ep, Remaining: 26.4h\n",
      "Episode 142: Reward = 210.0, Avg (last 10) = 183.5, Steps = 334929\n",
      "  Time: 111.0s/ep, Remaining: 26.4h\n",
      "Episode 143: Reward = 210.0, Avg (last 10) = 173.5, Steps = 337426\n",
      "  Time: 111.1s/ep, Remaining: 26.4h\n",
      "Episode 144: Reward = 105.0, Avg (last 10) = 173.5, Steps = 339389\n",
      "  Time: 110.9s/ep, Remaining: 26.3h\n",
      "Episode 145: Reward = 105.0, Avg (last 10) = 170.5, Steps = 341422\n",
      "  Time: 110.8s/ep, Remaining: 26.3h\n",
      "Episode 146: Reward = 160.0, Avg (last 10) = 176.0, Steps = 343963\n",
      "  Time: 110.9s/ep, Remaining: 26.3h\n",
      "Episode 147: Reward = 120.0, Avg (last 10) = 173.0, Steps = 346184\n",
      "  Time: 110.9s/ep, Remaining: 26.2h\n",
      "Episode 148: Reward = 185.0, Avg (last 10) = 162.0, Steps = 348597\n",
      "  Time: 110.9s/ep, Remaining: 26.2h\n",
      "Episode 149: Reward = 155.0, Avg (last 10) = 159.5, Steps = 350968\n",
      "  Time: 110.9s/ep, Remaining: 26.2h\n",
      "Episode 150: Reward = 55.0, Avg (last 10) = 144.0, Steps = 352287\n",
      "  Time: 110.6s/ep, Remaining: 26.1h\n",
      "ðŸ’¾ Checkpoint saved at episode 150\n",
      "Episode 151: Reward = 410.0, Avg (last 10) = 171.5, Steps = 354836\n",
      "  Time: 110.6s/ep, Remaining: 26.1h\n",
      "Episode 152: Reward = 260.0, Avg (last 10) = 176.5, Steps = 357667\n",
      "  Time: 110.8s/ep, Remaining: 26.1h\n",
      "Episode 153: Reward = 180.0, Avg (last 10) = 173.5, Steps = 360148\n",
      "  Time: 110.8s/ep, Remaining: 26.0h\n",
      "Episode 154: Reward = 230.0, Avg (last 10) = 186.0, Steps = 363101\n",
      "  Time: 111.0s/ep, Remaining: 26.1h\n",
      "Episode 155: Reward = 155.0, Avg (last 10) = 191.0, Steps = 365298\n",
      "  Time: 111.0s/ep, Remaining: 26.0h\n",
      "Episode 156: Reward = 105.0, Avg (last 10) = 185.5, Steps = 367217\n",
      "  Time: 110.8s/ep, Remaining: 26.0h\n",
      "Episode 157: Reward = 120.0, Avg (last 10) = 185.5, Steps = 369574\n",
      "  Time: 110.8s/ep, Remaining: 25.9h\n",
      "Episode 158: Reward = 440.0, Avg (last 10) = 211.0, Steps = 372327\n",
      "  Time: 110.9s/ep, Remaining: 25.9h\n",
      "Episode 159: Reward = 105.0, Avg (last 10) = 206.0, Steps = 374504\n",
      "  Time: 110.9s/ep, Remaining: 25.9h\n",
      "Episode 160: Reward = 180.0, Avg (last 10) = 218.5, Steps = 376871\n",
      "  Time: 110.9s/ep, Remaining: 25.8h\n",
      "Episode 161: Reward = 60.0, Avg (last 10) = 183.5, Steps = 378044\n",
      "  Time: 110.6s/ep, Remaining: 25.7h\n",
      "Episode 162: Reward = 130.0, Avg (last 10) = 170.5, Steps = 380335\n",
      "  Time: 110.5s/ep, Remaining: 25.7h\n",
      "Episode 163: Reward = 110.0, Avg (last 10) = 163.5, Steps = 382258\n",
      "  Time: 110.4s/ep, Remaining: 25.6h\n",
      "Episode 164: Reward = 135.0, Avg (last 10) = 154.0, Steps = 384577\n",
      "  Time: 110.4s/ep, Remaining: 25.6h\n",
      "Episode 165: Reward = 380.0, Avg (last 10) = 176.5, Steps = 387360\n",
      "  Time: 110.5s/ep, Remaining: 25.6h\n",
      "Episode 166: Reward = 545.0, Avg (last 10) = 220.5, Steps = 391301\n",
      "  Time: 111.0s/ep, Remaining: 25.7h\n",
      "ðŸ’¾ New best model saved! Reward: 545.0\n",
      "Episode 167: Reward = 135.0, Avg (last 10) = 222.0, Steps = 393564\n",
      "  Time: 111.0s/ep, Remaining: 25.6h\n",
      "Episode 168: Reward = 110.0, Avg (last 10) = 189.0, Steps = 395415\n",
      "  Time: 110.8s/ep, Remaining: 25.6h\n",
      "Episode 169: Reward = 355.0, Avg (last 10) = 214.0, Steps = 397906\n",
      "  Time: 110.9s/ep, Remaining: 25.6h\n",
      "Episode 170: Reward = 180.0, Avg (last 10) = 214.0, Steps = 400417\n",
      "  Time: 110.9s/ep, Remaining: 25.5h\n",
      "Episode 171: Reward = 110.0, Avg (last 10) = 219.0, Steps = 402442\n",
      "  Time: 110.8s/ep, Remaining: 25.5h\n",
      "Episode 172: Reward = 155.0, Avg (last 10) = 221.5, Steps = 404819\n",
      "  Time: 110.9s/ep, Remaining: 25.5h\n",
      "Episode 173: Reward = 260.0, Avg (last 10) = 236.5, Steps = 407692\n",
      "  Time: 111.0s/ep, Remaining: 25.5h\n",
      "Episode 174: Reward = 105.0, Avg (last 10) = 233.5, Steps = 409755\n",
      "  Time: 110.9s/ep, Remaining: 25.4h\n",
      "Episode 175: Reward = 55.0, Avg (last 10) = 201.0, Steps = 411708\n",
      "  Time: 110.8s/ep, Remaining: 25.4h\n",
      "Episode 176: Reward = 180.0, Avg (last 10) = 164.5, Steps = 414207\n",
      "  Time: 110.9s/ep, Remaining: 25.3h\n",
      "Episode 177: Reward = 110.0, Avg (last 10) = 162.0, Steps = 416320\n",
      "  Time: 110.8s/ep, Remaining: 25.3h\n",
      "Episode 178: Reward = 140.0, Avg (last 10) = 165.0, Steps = 418595\n",
      "  Time: 110.8s/ep, Remaining: 25.3h\n",
      "Episode 179: Reward = 160.0, Avg (last 10) = 145.5, Steps = 420946\n",
      "  Time: 110.8s/ep, Remaining: 25.2h\n",
      "Episode 180: Reward = 135.0, Avg (last 10) = 141.0, Steps = 423143\n",
      "  Time: 110.8s/ep, Remaining: 25.2h\n",
      "Episode 181: Reward = 45.0, Avg (last 10) = 134.5, Steps = 424566\n",
      "  Time: 110.5s/ep, Remaining: 25.1h\n",
      "Episode 182: Reward = 120.0, Avg (last 10) = 131.0, Steps = 427041\n",
      "  Time: 110.6s/ep, Remaining: 25.1h\n",
      "Episode 183: Reward = 380.0, Avg (last 10) = 143.0, Steps = 429690\n",
      "  Time: 110.7s/ep, Remaining: 25.1h\n",
      "Episode 184: Reward = 255.0, Avg (last 10) = 158.0, Steps = 432571\n",
      "  Time: 110.8s/ep, Remaining: 25.1h\n",
      "Episode 185: Reward = 180.0, Avg (last 10) = 170.5, Steps = 434950\n",
      "  Time: 110.8s/ep, Remaining: 25.1h\n",
      "Episode 186: Reward = 210.0, Avg (last 10) = 173.5, Steps = 437519\n",
      "  Time: 110.9s/ep, Remaining: 25.0h\n",
      "Episode 187: Reward = 185.0, Avg (last 10) = 181.0, Steps = 439960\n",
      "  Time: 110.9s/ep, Remaining: 25.0h\n",
      "Episode 188: Reward = 210.0, Avg (last 10) = 188.0, Steps = 442333\n",
      "  Time: 110.9s/ep, Remaining: 25.0h\n",
      "Episode 189: Reward = 135.0, Avg (last 10) = 185.5, Steps = 444672\n",
      "  Time: 110.9s/ep, Remaining: 25.0h\n",
      "Episode 190: Reward = 210.0, Avg (last 10) = 193.0, Steps = 447227\n",
      "  Time: 111.0s/ep, Remaining: 24.9h\n",
      "Episode 191: Reward = 120.0, Avg (last 10) = 200.5, Steps = 449588\n",
      "  Time: 111.0s/ep, Remaining: 24.9h\n",
      "Episode 192: Reward = 105.0, Avg (last 10) = 199.0, Steps = 451529\n",
      "  Time: 110.9s/ep, Remaining: 24.9h\n",
      "Episode 193: Reward = 180.0, Avg (last 10) = 179.0, Steps = 454086\n",
      "  Time: 110.9s/ep, Remaining: 24.8h\n",
      "Episode 194: Reward = 210.0, Avg (last 10) = 174.5, Steps = 456531\n",
      "  Time: 110.9s/ep, Remaining: 24.8h\n",
      "Episode 195: Reward = 135.0, Avg (last 10) = 170.0, Steps = 458892\n",
      "  Time: 111.0s/ep, Remaining: 24.8h\n",
      "Episode 196: Reward = 155.0, Avg (last 10) = 164.5, Steps = 461263\n",
      "  Time: 111.0s/ep, Remaining: 24.7h\n",
      "Episode 197: Reward = 715.0, Avg (last 10) = 217.5, Steps = 465152\n",
      "  Time: 111.3s/ep, Remaining: 24.8h\n",
      "ðŸ’¾ New best model saved! Reward: 715.0\n",
      "Episode 198: Reward = 110.0, Avg (last 10) = 207.5, Steps = 467055\n",
      "  Time: 111.2s/ep, Remaining: 24.7h\n",
      "Episode 199: Reward = 210.0, Avg (last 10) = 215.0, Steps = 469474\n",
      "  Time: 111.3s/ep, Remaining: 24.7h\n",
      "Episode 200: Reward = 50.0, Avg (last 10) = 199.0, Steps = 470683\n",
      "  Time: 111.0s/ep, Remaining: 24.6h\n",
      "ðŸ’¾ Checkpoint saved at episode 200\n",
      "Episode 201: Reward = 180.0, Avg (last 10) = 205.0, Steps = 473236\n",
      "  Time: 111.0s/ep, Remaining: 24.6h\n",
      "Episode 202: Reward = 75.0, Avg (last 10) = 202.0, Steps = 475001\n",
      "  Time: 110.9s/ep, Remaining: 24.6h\n",
      "Episode 203: Reward = 105.0, Avg (last 10) = 194.5, Steps = 477110\n",
      "  Time: 110.8s/ep, Remaining: 24.5h\n",
      "Episode 204: Reward = 440.0, Avg (last 10) = 217.5, Steps = 480487\n",
      "  Time: 111.1s/ep, Remaining: 24.5h\n",
      "Episode 205: Reward = 310.0, Avg (last 10) = 235.0, Steps = 484416\n",
      "  Time: 111.5s/ep, Remaining: 24.6h\n",
      "Episode 206: Reward = 380.0, Avg (last 10) = 257.5, Steps = 486921\n",
      "  Time: 111.5s/ep, Remaining: 24.6h\n",
      "Episode 207: Reward = 110.0, Avg (last 10) = 197.0, Steps = 488808\n",
      "  Time: 111.4s/ep, Remaining: 24.5h\n",
      "Episode 208: Reward = 65.0, Avg (last 10) = 192.5, Steps = 490323\n",
      "  Time: 111.2s/ep, Remaining: 24.4h\n",
      "Episode 209: Reward = 105.0, Avg (last 10) = 182.0, Steps = 492250\n",
      "  Time: 111.1s/ep, Remaining: 24.4h\n",
      "Episode 210: Reward = 65.0, Avg (last 10) = 183.5, Steps = 493597\n",
      "  Time: 110.9s/ep, Remaining: 24.3h\n",
      "Episode 211: Reward = 110.0, Avg (last 10) = 176.5, Steps = 495862\n",
      "  Time: 110.9s/ep, Remaining: 24.3h\n",
      "Episode 212: Reward = 80.0, Avg (last 10) = 177.0, Steps = 497273\n",
      "  Time: 110.7s/ep, Remaining: 24.2h\n",
      "Episode 213: Reward = 210.0, Avg (last 10) = 187.5, Steps = 500056\n",
      "  Time: 110.8s/ep, Remaining: 24.2h\n",
      "Episode 214: Reward = 340.0, Avg (last 10) = 177.5, Steps = 502369\n",
      "  Time: 110.7s/ep, Remaining: 24.1h\n",
      "Episode 215: Reward = 60.0, Avg (last 10) = 152.5, Steps = 504160\n",
      "  Time: 110.6s/ep, Remaining: 24.1h\n",
      "Episode 216: Reward = 55.0, Avg (last 10) = 120.0, Steps = 505311\n",
      "  Time: 110.4s/ep, Remaining: 24.0h\n",
      "Episode 217: Reward = 140.0, Avg (last 10) = 123.0, Steps = 507514\n",
      "  Time: 110.3s/ep, Remaining: 24.0h\n",
      "Episode 218: Reward = 210.0, Avg (last 10) = 137.5, Steps = 509847\n",
      "  Time: 110.3s/ep, Remaining: 23.9h\n",
      "Episode 219: Reward = 135.0, Avg (last 10) = 140.5, Steps = 512152\n",
      "  Time: 110.3s/ep, Remaining: 23.9h\n",
      "Episode 220: Reward = 110.0, Avg (last 10) = 145.0, Steps = 514461\n",
      "  Time: 110.3s/ep, Remaining: 23.9h\n",
      "Episode 221: Reward = 120.0, Avg (last 10) = 146.0, Steps = 516536\n",
      "  Time: 110.3s/ep, Remaining: 23.8h\n",
      "Episode 222: Reward = 335.0, Avg (last 10) = 171.5, Steps = 518847\n",
      "  Time: 110.3s/ep, Remaining: 23.8h\n",
      "Episode 223: Reward = 265.0, Avg (last 10) = 177.0, Steps = 521536\n",
      "  Time: 110.4s/ep, Remaining: 23.8h\n",
      "Episode 224: Reward = 210.0, Avg (last 10) = 164.0, Steps = 523981\n",
      "  Time: 110.4s/ep, Remaining: 23.8h\n",
      "Episode 225: Reward = 60.0, Avg (last 10) = 164.0, Steps = 525174\n",
      "  Time: 110.1s/ep, Remaining: 23.7h\n",
      "Episode 226: Reward = 120.0, Avg (last 10) = 170.5, Steps = 527127\n",
      "  Time: 110.1s/ep, Remaining: 23.6h\n",
      "Episode 227: Reward = 215.0, Avg (last 10) = 178.0, Steps = 529874\n",
      "  Time: 110.2s/ep, Remaining: 23.6h\n",
      "Episode 228: Reward = 210.0, Avg (last 10) = 178.0, Steps = 532171\n",
      "  Time: 110.2s/ep, Remaining: 23.6h\n",
      "Episode 229: Reward = 105.0, Avg (last 10) = 175.0, Steps = 533988\n",
      "  Time: 110.0s/ep, Remaining: 23.5h\n",
      "Episode 230: Reward = 80.0, Avg (last 10) = 172.0, Steps = 535129\n",
      "  Time: 109.8s/ep, Remaining: 23.5h\n",
      "Episode 231: Reward = 75.0, Avg (last 10) = 167.5, Steps = 536348\n",
      "  Time: 109.6s/ep, Remaining: 23.4h\n",
      "Episode 232: Reward = 285.0, Avg (last 10) = 162.5, Steps = 539349\n",
      "  Time: 109.7s/ep, Remaining: 23.4h\n",
      "Episode 233: Reward = 105.0, Avg (last 10) = 146.5, Steps = 541242\n",
      "  Time: 109.6s/ep, Remaining: 23.3h\n",
      "Episode 234: Reward = 215.0, Avg (last 10) = 147.0, Steps = 543999\n",
      "  Time: 109.7s/ep, Remaining: 23.3h\n",
      "Episode 235: Reward = 180.0, Avg (last 10) = 159.0, Steps = 546416\n",
      "  Time: 109.7s/ep, Remaining: 23.3h\n",
      "Episode 236: Reward = 270.0, Avg (last 10) = 174.0, Steps = 549489\n",
      "  Time: 109.9s/ep, Remaining: 23.3h\n",
      "Episode 237: Reward = 100.0, Avg (last 10) = 162.5, Steps = 551524\n",
      "  Time: 109.8s/ep, Remaining: 23.2h\n",
      "Episode 238: Reward = 210.0, Avg (last 10) = 162.5, Steps = 554641\n",
      "  Time: 110.0s/ep, Remaining: 23.3h\n",
      "Episode 239: Reward = 210.0, Avg (last 10) = 173.0, Steps = 557158\n",
      "  Time: 110.0s/ep, Remaining: 23.2h\n",
      "Episode 240: Reward = 160.0, Avg (last 10) = 181.0, Steps = 559513\n",
      "  Time: 110.0s/ep, Remaining: 23.2h\n",
      "Episode 241: Reward = 135.0, Avg (last 10) = 187.0, Steps = 561812\n",
      "  Time: 110.0s/ep, Remaining: 23.2h\n",
      "Episode 242: Reward = 105.0, Avg (last 10) = 169.0, Steps = 564097\n",
      "  Time: 110.0s/ep, Remaining: 23.1h\n",
      "Episode 243: Reward = 105.0, Avg (last 10) = 169.0, Steps = 566644\n",
      "  Time: 110.1s/ep, Remaining: 23.1h\n",
      "Episode 244: Reward = 135.0, Avg (last 10) = 161.0, Steps = 568975\n",
      "  Time: 110.1s/ep, Remaining: 23.1h\n",
      "Episode 245: Reward = 380.0, Avg (last 10) = 181.0, Steps = 571412\n",
      "  Time: 110.1s/ep, Remaining: 23.1h\n",
      "Episode 246: Reward = 180.0, Avg (last 10) = 172.0, Steps = 573843\n",
      "  Time: 110.1s/ep, Remaining: 23.0h\n",
      "Episode 247: Reward = 105.0, Avg (last 10) = 172.5, Steps = 575752\n",
      "  Time: 110.0s/ep, Remaining: 23.0h\n",
      "Episode 248: Reward = 155.0, Avg (last 10) = 167.0, Steps = 578163\n",
      "  Time: 110.1s/ep, Remaining: 23.0h\n",
      "Episode 249: Reward = 150.0, Avg (last 10) = 161.0, Steps = 580524\n",
      "  Time: 110.1s/ep, Remaining: 22.9h\n",
      "Episode 250: Reward = 210.0, Avg (last 10) = 166.0, Steps = 582909\n",
      "  Time: 110.1s/ep, Remaining: 22.9h\n",
      "ðŸ’¾ Checkpoint saved at episode 250\n",
      "Episode 251: Reward = 135.0, Avg (last 10) = 166.0, Steps = 585304\n",
      "  Time: 110.1s/ep, Remaining: 22.9h\n",
      "Episode 252: Reward = 180.0, Avg (last 10) = 173.5, Steps = 587703\n",
      "  Time: 110.1s/ep, Remaining: 22.8h\n",
      "Episode 253: Reward = 410.0, Avg (last 10) = 204.0, Steps = 590190\n",
      "  Time: 110.1s/ep, Remaining: 22.8h\n",
      "Episode 254: Reward = 155.0, Avg (last 10) = 206.0, Steps = 592511\n",
      "  Time: 110.1s/ep, Remaining: 22.8h\n",
      "Episode 255: Reward = 210.0, Avg (last 10) = 189.0, Steps = 595002\n",
      "  Time: 110.3s/ep, Remaining: 22.8h\n",
      "Episode 256: Reward = 185.0, Avg (last 10) = 189.5, Steps = 597415\n",
      "  Time: 110.4s/ep, Remaining: 22.8h\n",
      "Episode 257: Reward = 120.0, Avg (last 10) = 191.0, Steps = 599522\n",
      "  Time: 110.4s/ep, Remaining: 22.8h\n",
      "Episode 258: Reward = 105.0, Avg (last 10) = 186.0, Steps = 601445\n",
      "  Time: 110.5s/ep, Remaining: 22.7h\n",
      "Episode 259: Reward = 155.0, Avg (last 10) = 186.5, Steps = 604092\n",
      "  Time: 110.9s/ep, Remaining: 22.8h\n",
      "Episode 260: Reward = 75.0, Avg (last 10) = 173.0, Steps = 605303\n",
      "  Time: 110.8s/ep, Remaining: 22.7h\n",
      "Episode 261: Reward = 100.0, Avg (last 10) = 169.5, Steps = 606434\n",
      "  Time: 110.6s/ep, Remaining: 22.7h\n",
      "Episode 262: Reward = 105.0, Avg (last 10) = 162.0, Steps = 608519\n",
      "  Time: 110.5s/ep, Remaining: 22.6h\n",
      "Episode 263: Reward = 105.0, Avg (last 10) = 131.5, Steps = 610464\n",
      "  Time: 110.9s/ep, Remaining: 22.7h\n",
      "Episode 264: Reward = 245.0, Avg (last 10) = 140.5, Steps = 613239\n",
      "  Time: 112.8s/ep, Remaining: 23.0h\n",
      "Episode 265: Reward = 105.0, Avg (last 10) = 130.0, Steps = 615108\n",
      "  Time: 113.1s/ep, Remaining: 23.1h\n",
      "Episode 266: Reward = 240.0, Avg (last 10) = 135.5, Steps = 617999\n",
      "  Time: 113.1s/ep, Remaining: 23.0h\n",
      "Episode 267: Reward = 105.0, Avg (last 10) = 134.0, Steps = 619914\n",
      "  Time: 113.1s/ep, Remaining: 23.0h\n",
      "Episode 268: Reward = 210.0, Avg (last 10) = 144.5, Steps = 622375\n",
      "  Time: 113.6s/ep, Remaining: 23.1h\n",
      "Episode 269: Reward = 80.0, Avg (last 10) = 137.0, Steps = 623900\n",
      "  Time: 113.8s/ep, Remaining: 23.1h\n",
      "Episode 270: Reward = 105.0, Avg (last 10) = 140.0, Steps = 625873\n",
      "  Time: 113.8s/ep, Remaining: 23.0h\n",
      "Episode 271: Reward = 135.0, Avg (last 10) = 143.5, Steps = 628222\n",
      "  Time: 113.8s/ep, Remaining: 23.0h\n",
      "Episode 272: Reward = 135.0, Avg (last 10) = 146.5, Steps = 630739\n",
      "  Time: 115.0s/ep, Remaining: 23.2h\n",
      "Episode 273: Reward = 135.0, Avg (last 10) = 149.5, Steps = 633224\n",
      "  Time: 115.5s/ep, Remaining: 23.3h\n",
      "Episode 274: Reward = 210.0, Avg (last 10) = 146.0, Steps = 635797\n",
      "  Time: 116.2s/ep, Remaining: 23.4h\n",
      "Episode 275: Reward = 55.0, Avg (last 10) = 141.0, Steps = 637232\n",
      "  Time: 117.1s/ep, Remaining: 23.6h\n",
      "Episode 276: Reward = 135.0, Avg (last 10) = 130.5, Steps = 639569\n",
      "  Time: 118.8s/ep, Remaining: 23.9h\n",
      "Episode 277: Reward = 210.0, Avg (last 10) = 141.0, Steps = 642030\n",
      "  Time: 120.6s/ep, Remaining: 24.2h\n",
      "Episode 278: Reward = 155.0, Avg (last 10) = 135.5, Steps = 644441\n",
      "  Time: 122.2s/ep, Remaining: 24.5h\n",
      "Episode 279: Reward = 165.0, Avg (last 10) = 144.0, Steps = 646858\n",
      "  Time: 123.4s/ep, Remaining: 24.7h\n",
      "Episode 280: Reward = 135.0, Avg (last 10) = 147.0, Steps = 649221\n",
      "  Time: 124.1s/ep, Remaining: 24.8h\n",
      "Episode 281: Reward = 210.0, Avg (last 10) = 154.5, Steps = 651724\n",
      "  Time: 124.0s/ep, Remaining: 24.7h\n",
      "Episode 282: Reward = 155.0, Avg (last 10) = 156.5, Steps = 654183\n",
      "  Time: 124.0s/ep, Remaining: 24.7h\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 764\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;66;03m#wandb.finish()\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 764\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 659\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(resume_from, max_episodes)\u001b[0m\n\u001b[0;32m    657\u001b[0m batch \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch:\n\u001b[1;32m--> 659\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m#wandb.log(metrics, step=global_step)\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     \n\u001b[0;32m    662\u001b[0m     \u001b[38;5;66;03m# Log reconstruction occasionally\u001b[39;00m\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[12], line 293\u001b[0m, in \u001b[0;36mDreamerV3Agent.train_batch\u001b[1;34m(self, obs, actions, rewards, dones)\u001b[0m\n\u001b[0;32m    290\u001b[0m states, priors, posts \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[1;32m--> 293\u001b[0m     state, prior, post \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrssm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[0;32m    295\u001b[0m     priors\u001b[38;5;241m.\u001b[39mappend(prior)\n",
      "Cell \u001b[1;32mIn[12], line 160\u001b[0m, in \u001b[0;36mRSSM.observe\u001b[1;34m(self, embed, action, state)\u001b[0m\n\u001b[0;32m    157\u001b[0m post_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dist(post_logits)\n\u001b[0;32m    158\u001b[0m stoch \u001b[38;5;241m=\u001b[39m post_dist\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;241m+\u001b[39m post_dist\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;241m-\u001b[39m post_dist\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m--> 160\u001b[0m prior_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprior_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeter\u001b[39m\u001b[38;5;124m'\u001b[39m: deter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstoch\u001b[39m\u001b[38;5;124m'\u001b[39m: stoch}, prior_logits, post_logits\n",
      "File \u001b[1;32md:\\Fall25\\RL\\Assignment-5\\Reinforcement-Learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Fall25\\RL\\Assignment-5\\Reinforcement-Learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Fall25\\RL\\Assignment-5\\Reinforcement-Learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Fall25\\RL\\Assignment-5\\Reinforcement-Learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Fall25\\RL\\Assignment-5\\Reinforcement-Learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Fall25\\RL\\Assignment-5\\Reinforcement-Learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Fall25\\RL\\Assignment-5\\Reinforcement-Learning\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2892\u001b[0m         layer_norm,\n\u001b[0;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2899\u001b[0m     )\n\u001b[1;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import OneHotCategorical\n",
    "from torch.distributions.kl import kl_divergence\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import ale_py\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Utility Functions\n",
    "# ============================================================================\n",
    "\n",
    "def symlog(x):\n",
    "    return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
    "\n",
    "def lambda_return(rewards, values, continues, bootstrap, lambda_=0.95):\n",
    "    \"\"\"Compute lambda returns properly.\"\"\"\n",
    "    # rewards: [B, H], values: [B, H], continues: [B, H], bootstrap: [B]\n",
    "    next_values = torch.cat([values[:, 1:], bootstrap[:, None]], dim=1)\n",
    "    targets = rewards + continues * next_values\n",
    "    \n",
    "    outputs = []\n",
    "    last = bootstrap\n",
    "    for t in reversed(range(rewards.shape[1])):\n",
    "        last = targets[:, t] + continues[:, t] * lambda_ * (last - next_values[:, t])\n",
    "        outputs.append(last)\n",
    "    \n",
    "    returns = torch.stack(list(reversed(outputs)), dim=1)\n",
    "    return returns\n",
    "\n",
    "# ============================================================================\n",
    "# Neural Network Components\n",
    "# ============================================================================\n",
    "\n",
    "class Conv2dSame(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.stride == 1:\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "            x = F.pad(x, [pad, pad, pad, pad])\n",
    "        return self.conv(x)\n",
    "\n",
    "class ConvTranspose2dSame(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        if self.stride == 2:\n",
    "            pad = 1\n",
    "            out = out[:, :, pad:-pad, pad:-pad]\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, out_dim=1024):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, stride=2),   # 64 â†’ 31\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),  # 31 â†’ 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2), # 14 â†’ 6\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 4, stride=2),# 6 â†’ 2\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(128 * 2 * 2, out_dim)  \n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = obs / 255.0\n",
    "        x = self.conv(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, feat_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(feat_dim, 1024)\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 128, 5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 6, stride=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, feat):\n",
    "        x = self.fc(feat)\n",
    "        x = x.view(-1, 1024, 1, 1)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "class RSSM(nn.Module):\n",
    "    def __init__(self, action_dim, embed_dim, deter_size=1024, stoch_size=32, hidden_size=512, num_classes=32):\n",
    "        super().__init__()\n",
    "        self.deter_size = deter_size\n",
    "        self.stoch_size = stoch_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.prior_net = nn.Sequential(\n",
    "            nn.Linear(deter_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, stoch_size * num_classes)\n",
    "        )\n",
    "        \n",
    "        # Now using the passed embed_dim\n",
    "        self.posterior_net = nn.Sequential(\n",
    "            nn.Linear(deter_size + embed_dim, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, stoch_size * num_classes)\n",
    "        )\n",
    "        \n",
    "        self.gru = nn.GRUCell(stoch_size * num_classes + action_dim, deter_size)\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_size).to(device),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_size, self.num_classes).to(device)\n",
    "        }\n",
    "    \n",
    "    def get_dist(self, logits):\n",
    "        logits = logits.reshape(*logits.shape[:-1], self.stoch_size, self.num_classes)\n",
    "        return OneHotCategorical(logits=logits)\n",
    "    \n",
    "    def observe(self, embed, action, state):\n",
    "        stoch_flat = state['stoch'].reshape(state['stoch'].shape[0], -1)\n",
    "        deter = self.gru(torch.cat([stoch_flat, action], dim=-1), state['deter'])\n",
    "        \n",
    "        post_logits = self.posterior_net(torch.cat([deter, embed], dim=-1))\n",
    "        post_dist = self.get_dist(post_logits)\n",
    "        stoch = post_dist.sample() + post_dist.probs - post_dist.probs.detach()\n",
    "        \n",
    "        prior_logits = self.prior_net(deter)\n",
    "        \n",
    "        return {'deter': deter, 'stoch': stoch}, prior_logits, post_logits\n",
    "    \n",
    "    def imagine(self, action, state):\n",
    "        stoch_flat = state['stoch'].reshape(state['stoch'].shape[0], -1)\n",
    "        deter = self.gru(torch.cat([stoch_flat, action], dim=-1), state['deter'])\n",
    "        \n",
    "        prior_logits = self.prior_net(deter)\n",
    "        prior_dist = self.get_dist(prior_logits)\n",
    "        stoch = prior_dist.sample() + prior_dist.probs - prior_dist.probs.detach()\n",
    "        \n",
    "        return {'deter': deter, 'stoch': stoch}\n",
    "    \n",
    "    def get_feat(self, state):\n",
    "        stoch = state['stoch']\n",
    "        deter = state['deter']\n",
    "        \n",
    "        # Handle both single step [B, D] and sequence [B, T, D] cases\n",
    "        if stoch.dim() == 3:  # [B, stoch_size, num_classes]\n",
    "            stoch_flat = stoch.reshape(stoch.shape[0], -1)\n",
    "        elif stoch.dim() == 4:  # [B, T, stoch_size, num_classes]\n",
    "            stoch_flat = stoch.reshape(*stoch.shape[:2], -1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected stoch dimensions: {stoch.shape}\")\n",
    "        \n",
    "        return torch.cat([deter, stoch_flat], dim=-1)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, feat_size, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, feat):\n",
    "        return OneHotCategorical(logits=self.net(feat))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, feat_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, feat):\n",
    "        return self.net(feat).squeeze(-1)\n",
    "\n",
    "# ============================================================================\n",
    "# DreamerV3 Agent\n",
    "# ============================================================================\n",
    "\n",
    "class DreamerV3Agent:\n",
    "    def __init__(self, action_dim, obs_shape=(3, 64, 64), lr=3e-4):\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Create encoder to get actual embed dimension\n",
    "        self.encoder = Encoder().to(device)\n",
    "        \n",
    "        # Get embed dimension by doing a forward pass\n",
    "        with torch.no_grad():\n",
    "            dummy_obs = torch.zeros(1, *obs_shape).to(device)\n",
    "            embed_dim = self.encoder(dummy_obs).shape[-1]\n",
    "        \n",
    "        print(f\"ðŸ“ Encoder output dimension: {embed_dim}\")\n",
    "        \n",
    "        feat_size = 1024 + 32*32  # deter_size + stoch_size * num_classes\n",
    "        \n",
    "        self.rssm = RSSM(action_dim, embed_dim=embed_dim).to(device)\n",
    "        self.decoder = Decoder(feat_size).to(device)\n",
    "        self.reward_pred = Critic(feat_size).to(device)\n",
    "        self.continue_pred = Critic(feat_size).to(device)\n",
    "        self.actor = Actor(feat_size, action_dim).to(device)\n",
    "        self.critic = Critic(feat_size).to(device)\n",
    "        \n",
    "        world_params = (list(self.encoder.parameters()) + \n",
    "                       list(self.rssm.parameters()) + \n",
    "                       list(self.decoder.parameters()) +\n",
    "                       list(self.reward_pred.parameters()) +\n",
    "                       list(self.continue_pred.parameters()))\n",
    "        \n",
    "        self.world_opt = optim.Adam(world_params, lr=lr)\n",
    "        self.actor_opt = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_opt = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "    \n",
    "    def act(self, obs, state, training=True):\n",
    "        with torch.no_grad():\n",
    "            embed = self.encoder(torch.FloatTensor(obs).unsqueeze(0).to(device))\n",
    "            \n",
    "            if state is None:\n",
    "                state = self.rssm.initial_state(1)\n",
    "            \n",
    "            action_dummy = torch.zeros(1, self.action_dim).to(device)\n",
    "            state, _, _ = self.rssm.observe(embed, action_dummy, state)\n",
    "            \n",
    "            feat = self.rssm.get_feat(state)\n",
    "            action_dist = self.actor(feat)\n",
    "            \n",
    "            if training:\n",
    "                action = action_dist.sample()\n",
    "            else:\n",
    "                action = F.one_hot(action_dist.probs.argmax(dim=-1), self.action_dim).float()\n",
    "            \n",
    "            return action.cpu().numpy()[0].argmax(), state\n",
    "    \n",
    "    def train_batch(self, obs, actions, rewards, dones):\n",
    "        B, T = obs.shape[:2]\n",
    "    \n",
    "        # =====================================================\n",
    "        # Encode observations\n",
    "        # =====================================================\n",
    "        embed = self.encoder(obs.reshape(B * T, *obs.shape[2:]))\n",
    "        embed = embed.reshape(B, T, -1)\n",
    "    \n",
    "        # =====================================================\n",
    "        # World Model Rollout\n",
    "        # =====================================================\n",
    "        state = self.rssm.initial_state(B)\n",
    "        states, priors, posts = [], [], []\n",
    "    \n",
    "        for t in range(T):\n",
    "            state, prior, post = self.rssm.observe(embed[:, t], actions[:, t], state)\n",
    "            states.append(state)\n",
    "            priors.append(prior)\n",
    "            posts.append(post)\n",
    "    \n",
    "        deter = torch.stack([s[\"deter\"] for s in states], dim=1)\n",
    "        stoch = torch.stack([s[\"stoch\"] for s in states], dim=1)\n",
    "    \n",
    "        feat = self.rssm.get_feat({\"deter\": deter, \"stoch\": stoch})\n",
    "    \n",
    "        # =====================================================\n",
    "        # World Model Losses\n",
    "        # =====================================================\n",
    "        obs_flat = obs.reshape(B * T, *obs.shape[2:]) / 255.0\n",
    "        recon = self.decoder(feat.reshape(B * T, -1))\n",
    "        recon_loss = F.mse_loss(recon, obs_flat)\n",
    "    \n",
    "        feat_flat = feat.reshape(B * T, -1)\n",
    "        pred_reward = self.reward_pred(feat_flat).reshape(B, T)\n",
    "        reward_loss = F.mse_loss(pred_reward, rewards)\n",
    "    \n",
    "        pred_continue = self.continue_pred(feat_flat).reshape(B, T)\n",
    "        continue_target = 1.0 - dones.float()\n",
    "        continue_loss = F.binary_cross_entropy_with_logits(\n",
    "            pred_continue, continue_target\n",
    "        )\n",
    "    \n",
    "        prior_dist = self.rssm.get_dist(torch.stack(priors, 1))\n",
    "        post_dist = self.rssm.get_dist(torch.stack(posts, 1))\n",
    "        kl = kl_divergence(post_dist, prior_dist).mean()\n",
    "        kl_loss = torch.maximum(kl, torch.tensor(1.0, device=device))\n",
    "    \n",
    "        world_loss = recon_loss + reward_loss + continue_loss + kl_loss\n",
    "    \n",
    "        self.world_opt.zero_grad()\n",
    "        world_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(self.encoder.parameters())\n",
    "            + list(self.rssm.parameters())\n",
    "            + list(self.decoder.parameters())\n",
    "            + list(self.reward_pred.parameters())\n",
    "            + list(self.continue_pred.parameters()),\n",
    "            100.0,\n",
    "        )\n",
    "        self.world_opt.step()\n",
    "    \n",
    "        # =====================================================\n",
    "        # Imagination (NO GRAD STOP HERE)\n",
    "        # =====================================================\n",
    "        with torch.no_grad():\n",
    "            imag_state = {\n",
    "                \"deter\": deter[:, -1],\n",
    "                \"stoch\": stoch[:, -1],\n",
    "            }\n",
    "    \n",
    "        imag_states = [imag_state]\n",
    "        horizon = 15\n",
    "    \n",
    "        for _ in range(horizon):\n",
    "            feat_i = self.rssm.get_feat(imag_state)\n",
    "            action = self.actor(feat_i).sample()\n",
    "            imag_state = self.rssm.imagine(action, imag_state)\n",
    "            imag_states.append(imag_state)\n",
    "    \n",
    "        imag_deter = torch.stack([s[\"deter\"] for s in imag_states], dim=1)\n",
    "        imag_stoch = torch.stack([s[\"stoch\"] for s in imag_states], dim=1)\n",
    "        imag_feat = self.rssm.get_feat({\"deter\": imag_deter, \"stoch\": imag_stoch})\n",
    "    \n",
    "        # =====================================================\n",
    "        # Actor Update (USES IMAGINATION GRAPH)\n",
    "        # =====================================================\n",
    "        imag_feat_flat = imag_feat.reshape(-1, imag_feat.shape[-1])\n",
    "        imag_reward = self.reward_pred(imag_feat_flat).reshape(B, horizon + 1)\n",
    "        imag_value = self.critic(imag_feat_flat).reshape(B, horizon + 1)\n",
    "        imag_continue = torch.sigmoid(\n",
    "            self.continue_pred(imag_feat_flat)\n",
    "        ).reshape(B, horizon + 1)\n",
    "    \n",
    "        returns = lambda_return(\n",
    "            imag_reward[:, :-1],\n",
    "            imag_value[:, :-1],\n",
    "            imag_continue[:, :-1],\n",
    "            imag_value[:, -1],\n",
    "            lambda_=0.95,\n",
    "        )\n",
    "    \n",
    "        discount = torch.cumprod(\n",
    "            torch.cat(\n",
    "                [torch.ones(B, 1, device=device), imag_continue[:, :-1]], dim=1\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "    \n",
    "        actor_loss = -(discount[:, :-1] * returns.detach()).mean()\n",
    "    \n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 100.0)\n",
    "        self.actor_opt.step()\n",
    "    \n",
    "        # =====================================================\n",
    "        # Critic Update (NEW GRAPH â€” DETACHED)\n",
    "        # =====================================================\n",
    "        imag_feat_detached = imag_feat.detach()\n",
    "        imag_feat_flat_detached = imag_feat_detached.reshape(\n",
    "            -1, imag_feat_detached.shape[-1]\n",
    "        )\n",
    "    \n",
    "        imag_value_detached = self.critic(imag_feat_flat_detached).reshape(\n",
    "            B, horizon + 1\n",
    "        )\n",
    "    \n",
    "        critic_loss = F.mse_loss(\n",
    "            imag_value_detached[:, :-1], returns.detach()\n",
    "        )\n",
    "    \n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 100.0)\n",
    "        self.critic_opt.step()\n",
    "    \n",
    "        return {\n",
    "            \"world_loss\": world_loss.item(),\n",
    "            \"recon_loss\": recon_loss.item(),\n",
    "            \"reward_loss\": reward_loss.item(),\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "            \"actor_loss\": actor_loss.item(),\n",
    "            \"critic_loss\": critic_loss.item(),\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# Replay Buffer\n",
    "# ============================================================================\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=50000, seq_len=50):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def add(self, obs, act, rew, done):\n",
    "        self.buffer.append((obs, act, rew, done))\n",
    "        if len(self.buffer) > self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) < self.seq_len + 1:\n",
    "            return None\n",
    "        \n",
    "        obs_b, act_b, rew_b, done_b = [], [], [], []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            idx = random.randint(0, len(self.buffer) - self.seq_len - 1)\n",
    "            seq = self.buffer[idx:idx + self.seq_len]\n",
    "            \n",
    "            obs_b.append(np.stack([s[0] for s in seq]))\n",
    "            act_b.append(np.array([s[1] for s in seq]))\n",
    "            rew_b.append(np.array([s[2] for s in seq]))\n",
    "            done_b.append(np.array([s[3] for s in seq]))\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.stack(obs_b)).to(device),\n",
    "            torch.FloatTensor(np.stack(act_b)).to(device),\n",
    "            torch.FloatTensor(np.stack(rew_b)).to(device),\n",
    "            torch.FloatTensor(np.stack(done_b)).to(device)\n",
    "        )\n",
    "\n",
    "def preprocess_obs(obs):\n",
    "    img = Image.fromarray(obs).resize((64, 64), Image.BILINEAR)\n",
    "    return np.transpose(np.array(img), (2, 0, 1))\n",
    "\n",
    "# ============================================================================\n",
    "# Load and Evaluate Model\n",
    "# ============================================================================\n",
    "\n",
    "def load_model(checkpoint_path, action_dim):\n",
    "    \"\"\"Load a saved model for evaluation or continued training.\"\"\"\n",
    "    agent = DreamerV3Agent(action_dim=action_dim)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    agent.actor.load_state_dict(checkpoint['actor'])\n",
    "    agent.critic.load_state_dict(checkpoint['critic'])\n",
    "    agent.encoder.load_state_dict(checkpoint['encoder'])\n",
    "    agent.rssm.load_state_dict(checkpoint['rssm'])\n",
    "    \n",
    "    if 'decoder' in checkpoint:\n",
    "        agent.decoder.load_state_dict(checkpoint['decoder'])\n",
    "    if 'reward_pred' in checkpoint:\n",
    "        agent.reward_pred.load_state_dict(checkpoint['reward_pred'])\n",
    "    if 'continue_pred' in checkpoint:\n",
    "        agent.continue_pred.load_state_dict(checkpoint['continue_pred'])\n",
    "    \n",
    "    print(f\"âœ… Model loaded from {checkpoint_path}\")\n",
    "    if 'episode' in checkpoint:\n",
    "        print(f\"   Episode: {checkpoint['episode']}, Reward: {checkpoint.get('reward', 'N/A')}\")\n",
    "    \n",
    "    return agent, checkpoint\n",
    "\n",
    "def evaluate_model(checkpoint_path, num_episodes=10):\n",
    "    \"\"\"Evaluate a saved model.\"\"\"\n",
    "    # Try different environment names\n",
    "    env_names = ['SpaceInvadersNoFrameskip-v4', 'SpaceInvaders-v4', 'SpaceInvaders-v0', 'ALE/SpaceInvaders-v5']\n",
    "    env = None\n",
    "    for env_name in env_names:\n",
    "        try:\n",
    "            env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "            print(f\"âœ… Using environment: {env_name}\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if env is None:\n",
    "        raise RuntimeError(\"Could not create environment\")\n",
    "    \n",
    "    agent, checkpoint = load_model(checkpoint_path, env.action_space.n)\n",
    "    \n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs = preprocess_obs(obs)\n",
    "        state = None\n",
    "        ep_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action_idx, state = agent.act(obs, state, training=False)\n",
    "            next_obs, reward, term, trunc, _ = env.step(action_idx)\n",
    "            done = term or trunc\n",
    "            next_obs = preprocess_obs(next_obs)\n",
    "            obs = next_obs\n",
    "            ep_reward += reward\n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {ep_reward:.1f}\")\n",
    "    \n",
    "    print(f\"\\nAverage Reward: {np.mean(rewards):.1f} Â± {np.std(rewards):.1f}\")\n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "# ============================================================================\n",
    "# Training Loop\n",
    "# ============================================================================\n",
    "\n",
    "def train(resume_from=None, max_episodes=300):\n",
    "    \"\"\"\n",
    "    Train the DreamerV3 agent.\n",
    "    \n",
    "    Args:\n",
    "        resume_from: Path to checkpoint to resume from (e.g., \"checkpoints/recovery.pt\")\n",
    "        max_episodes: Maximum number of episodes to train (default: 300 for Kaggle)\n",
    "    \"\"\"\n",
    "    # Initialize wandb\n",
    "    # wandb.init(\n",
    "    #     project=\"mahmoud-dreamer-v3-spaceinvaders\",\n",
    "    #     config={\n",
    "    #         \"lr\": 3e-4,\n",
    "    #         \"horizon\": 15,\n",
    "    #         \"batch_size\": 16,\n",
    "    #         \"seq_len\": 50,\n",
    "    #         \"buffer_size\": 50000\n",
    "    #     },\n",
    "    #     resume=\"allow\" if resume_from else False\n",
    "    #     mode=\"disabled\"\n",
    "    # )\n",
    "    \n",
    "    env_name = 'SpaceInvadersNoFrameskip-v4'\n",
    "    \n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    # env = gym.wrappers.AtariPreprocessing(\n",
    "    #     env,\n",
    "    #     frame_skip=1,          \n",
    "    #     grayscale_obs=True,\n",
    "    #     screen_size=64\n",
    "    # )\n",
    "    print(f\"âœ… Successfully created environment: {env_name}\")\n",
    "    print(f\"   Action space: {env.action_space}\")\n",
    "    print(f\"   Observation space: {env.observation_space}\")\n",
    "\n",
    "    \n",
    "    if env is None:\n",
    "        print(f\"\\nâŒ Could not create any SpaceInvaders environment!\")\n",
    "        print(f\"Last error: {last_error}\")\n",
    "        print(\"\\nðŸ”§ Please run this in Cell 1:\")\n",
    "        print(\"!pip install -q 'gymnasium==0.29.1' 'ale-py==0.8.1' 'shimmy[atari]'\")\n",
    "        print(\"import ale_py\")\n",
    "        print(\"ale_py.roms.utils.import_roms()\")\n",
    "        raise RuntimeError(\"Environment creation failed\")\n",
    "    \n",
    "    try:\n",
    "        env = RecordVideo(env, \"./videos\", episode_trigger=lambda x: x % 50 == 0)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Video recording failed: {e}, continuing without recording...\")\n",
    "    \n",
    "    # Get observation shape from environment\n",
    "    obs_sample, _ = env.reset()\n",
    "    obs_preprocessed = preprocess_obs(obs_sample)\n",
    "    obs_shape = obs_preprocessed.shape\n",
    "    print(f\"ðŸ“ Preprocessed observation shape: {obs_shape}\")\n",
    "    \n",
    "    agent = DreamerV3Agent(action_dim=env.action_space.n, obs_shape=obs_shape)\n",
    "    buffer = ReplayBuffer()\n",
    "    \n",
    "    global_step = 0\n",
    "    episode_rewards = []\n",
    "    best_reward = -float('inf')\n",
    "    start_episode = 0\n",
    "    \n",
    "    # Resume from checkpoint if specified\n",
    "    if resume_from and os.path.exists(resume_from):\n",
    "        print(f\"ðŸ“‚ Resuming from {resume_from}\")\n",
    "        checkpoint = torch.load(resume_from, map_location=device)\n",
    "        agent.actor.load_state_dict(checkpoint['actor'])\n",
    "        agent.critic.load_state_dict(checkpoint['critic'])\n",
    "        agent.encoder.load_state_dict(checkpoint['encoder'])\n",
    "        agent.rssm.load_state_dict(checkpoint['rssm'])\n",
    "        if 'decoder' in checkpoint:\n",
    "            agent.decoder.load_state_dict(checkpoint['decoder'])\n",
    "        if 'reward_pred' in checkpoint:\n",
    "            agent.reward_pred.load_state_dict(checkpoint['reward_pred'])\n",
    "        if 'continue_pred' in checkpoint:\n",
    "            agent.continue_pred.load_state_dict(checkpoint['continue_pred'])\n",
    "        start_episode = checkpoint.get('episode', 0) + 1\n",
    "        episode_rewards = checkpoint.get('episode_rewards', [])\n",
    "        best_reward = checkpoint.get('best_reward', -float('inf'))\n",
    "        print(f\"   Resuming from episode {start_episode}\")\n",
    "        print(f\"   Best reward so far: {best_reward:.1f}\")\n",
    "    \n",
    "    # Create checkpoints directory\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    \n",
    "    # Time tracking\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for episode in range(start_episode, start_episode + max_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs = preprocess_obs(obs)\n",
    "        \n",
    "        state = None\n",
    "        ep_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action_idx, state = agent.act(obs, state, training=True)\n",
    "            act_onehot = np.zeros(env.action_space.n)\n",
    "            act_onehot[action_idx] = 1.0\n",
    "            \n",
    "            # Step environment\n",
    "            next_obs, reward, term, trunc, _ = env.step(action_idx)\n",
    "            done = term or trunc\n",
    "            next_obs = preprocess_obs(next_obs)\n",
    "            \n",
    "            # Store in buffer\n",
    "            buffer.add(obs, act_onehot, reward, done)\n",
    "            \n",
    "            obs = next_obs\n",
    "            ep_reward += reward\n",
    "            step += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            # Train\n",
    "            if step % 5 == 0 and len(buffer.buffer) > 1000:\n",
    "                batch = buffer.sample(16)\n",
    "                if batch:\n",
    "                    metrics = agent.train_batch(*batch)\n",
    "                    #wandb.log(metrics, step=global_step)\n",
    "                    \n",
    "                    # Log reconstruction occasionally\n",
    "                    if global_step % 500 == 0:\n",
    "                        with torch.no_grad():\n",
    "                            feat = agent.rssm.get_feat(state)\n",
    "                            recon = agent.decoder(feat)\n",
    "                            # wandb.log({\n",
    "                            #     \"reconstruction\": wandb.Image(\n",
    "                            #         recon[0].cpu().numpy().transpose(1, 2, 0)\n",
    "                            #     )\n",
    "                            # }, step=global_step)\n",
    "        \n",
    "        episode_rewards.append(ep_reward)\n",
    "        avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else ep_reward\n",
    "        \n",
    "        # Time estimation\n",
    "        elapsed_time = time.time() - start_time\n",
    "        episodes_done = episode - start_episode + 1\n",
    "        time_per_episode = elapsed_time / episodes_done\n",
    "        remaining_episodes = start_episode + max_episodes - episode - 1\n",
    "        estimated_remaining = time_per_episode * remaining_episodes\n",
    "        \n",
    "        # wandb.log({\n",
    "        #     \"episode_reward\": ep_reward,\n",
    "        #     \"avg_reward_10\": avg_reward,\n",
    "        #     \"episode\": episode,\n",
    "        #     \"time_per_episode\": time_per_episode,\n",
    "        # }, step=global_step)\n",
    "        \n",
    "        print(f\"Episode {episode}: Reward = {ep_reward:.1f}, \"\n",
    "              f\"Avg (last 10) = {avg_reward:.1f}, Steps = {global_step}\")\n",
    "        print(f\"  Time: {time_per_episode:.1f}s/ep, \"\n",
    "              f\"Remaining: {estimated_remaining/3600:.1f}h\")\n",
    "        \n",
    "        # Save best model\n",
    "        if ep_reward > best_reward:\n",
    "            best_reward = ep_reward\n",
    "            torch.save({\n",
    "                'episode': episode,\n",
    "                'reward': ep_reward,\n",
    "                'actor': agent.actor.state_dict(),\n",
    "                'critic': agent.critic.state_dict(),\n",
    "                'encoder': agent.encoder.state_dict(),\n",
    "                'rssm': agent.rssm.state_dict(),\n",
    "                'decoder': agent.decoder.state_dict(),\n",
    "                'reward_pred': agent.reward_pred.state_dict(),\n",
    "                'continue_pred': agent.continue_pred.state_dict(),\n",
    "            }, \"checkpoints/best_model.pt\")\n",
    "            print(f\"ðŸ’¾ New best model saved! Reward: {ep_reward:.1f}\")\n",
    "        \n",
    "        # Save regular checkpoint every 50 episodes\n",
    "        if episode % 50 == 0 and episode > 0:\n",
    "            torch.save({\n",
    "                'episode': episode,\n",
    "                'reward': ep_reward,\n",
    "                'actor': agent.actor.state_dict(),\n",
    "                'critic': agent.critic.state_dict(),\n",
    "                'encoder': agent.encoder.state_dict(),\n",
    "                'rssm': agent.rssm.state_dict(),\n",
    "                'decoder': agent.decoder.state_dict(),\n",
    "                'reward_pred': agent.reward_pred.state_dict(),\n",
    "                'continue_pred': agent.continue_pred.state_dict(),\n",
    "                'episode_rewards': episode_rewards,\n",
    "            }, f\"checkpoints/checkpoint_ep{episode}.pt\")\n",
    "            print(f\"ðŸ’¾ Checkpoint saved at episode {episode}\")\n",
    "        \n",
    "        # Save recovery checkpoint every 10 episodes (overwrites)\n",
    "        if episode % 10 == 0:\n",
    "            torch.save({\n",
    "                'episode': episode,\n",
    "                'reward': ep_reward,\n",
    "                'actor': agent.actor.state_dict(),\n",
    "                'critic': agent.critic.state_dict(),\n",
    "                'encoder': agent.encoder.state_dict(),\n",
    "                'rssm': agent.rssm.state_dict(),\n",
    "                'decoder': agent.decoder.state_dict(),\n",
    "                'reward_pred': agent.reward_pred.state_dict(),\n",
    "                'continue_pred': agent.continue_pred.state_dict(),\n",
    "                'episode_rewards': episode_rewards,\n",
    "                'best_reward': best_reward,\n",
    "            }, \"checkpoints/recovery.pt\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'reward': ep_reward,\n",
    "        'actor': agent.actor.state_dict(),\n",
    "        'critic': agent.critic.state_dict(),\n",
    "        'encoder': agent.encoder.state_dict(),\n",
    "        'rssm': agent.rssm.state_dict(),\n",
    "        'decoder': agent.decoder.state_dict(),\n",
    "        'reward_pred': agent.reward_pred.state_dict(),\n",
    "        'continue_pred': agent.continue_pred.state_dict(),\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'best_reward': best_reward,\n",
    "    }, \"checkpoints/final_model.pt\")\n",
    "    print(f\"Final model saved!\")\n",
    "    \n",
    "    env.close()\n",
    "    #wandb.finish()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    evaluate_model(\"checkpoints/best_model.pt\", num_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
